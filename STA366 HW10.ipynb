{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22521173",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f1645",
   "metadata": {},
   "source": [
    "A stochastic process is a collection of random variables indexed over time or space, describing systems that evolve randomly.A Gaussian Process (GP) is a specific type of stochastic process where any finite collection of variables follows a multivariate Gaussian distribution.GPs are useful for non-parametric regression and probabilistic modeling in machine learning.Variational Inference (VI) is a technique for approximating probability distributions, often used in Bayesian machine learning.ELBO (Evidence Lower Bound) is an objective function in VI that helps approximate an intractable posterior distribution. Instead of directly computing the posterior, VI optimizes ELBO to find a simpler approximation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9dd23",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "\n",
    "# Load real-world temperature dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
    "data = pd.read_csv(url, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "data = data.resample(\"M\").mean()  # Convert to monthly averages\n",
    "X = np.arange(len(data)).reshape(-1, 1)  # Time indices as feature\n",
    "y = data[\"Temp\"].values  # Temperature values\n",
    "\n",
    "# Standardize the target variable for numerical stability\n",
    "y_mean, y_std = y.mean(), y.std()\n",
    "y = (y - y_mean) / y_std  # Normalize temperature values\n",
    "\n",
    "# Define a Gaussian Process Model in PyMC\n",
    "with pm.Model() as gp_model:\n",
    "    # Define GP components\n",
    "    mean_func = pm.gp.mean.Zero()  # Zero mean function\n",
    "    cov_func = pm.gp.cov.ExpQuad(input_dim=1, ls=10)  # Squared exponential kernel\n",
    "    \n",
    "    # Define GP with keyword arguments\n",
    "    gp = pm.gp.Marginal(mean_func=mean_func, cov_func=cov_func)\n",
    "\n",
    "    # Likelihood (Noise term)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "    \n",
    "    # Define the Marginal Likelihood\n",
    "    y_obs = gp.marginal_likelihood(\"y_obs\", X=X, y=y, noise=sigma)\n",
    "\n",
    "    # Perform inference using MCMC\n",
    "    trace = pm.sample(1000, return_inferencedata=True, chains=2, target_accept=0.9)\n",
    "\n",
    "# Plot the results\n",
    "az.plot_trace(trace)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b105d",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load dataset (MNIST)\n",
    "(X_train, y_train), (_, _) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28 * 28) / 255.0  # Flatten images & normalize\n",
    "y_train = y_train.astype(\"int\")\n",
    "\n",
    "# Select a smaller dataset for fast training\n",
    "X_train, y_train = X_train[:1000], y_train[:1000]\n",
    "\n",
    "# Define a Bayesian Neural Network in PyMC\n",
    "with pm.Model() as bnn:\n",
    "    # Priors for the first layer\n",
    "    W1 = pm.Normal(\"W1\", 0, 1, shape=(28 * 28, 128))\n",
    "    b1 = pm.Normal(\"b1\", 0, 1, shape=(128,))\n",
    "    \n",
    "    # Hidden layer\n",
    "    hidden = pm.math.tanh(pm.math.dot(X_train, W1) + b1)\n",
    "    \n",
    "    # Priors for the second layer\n",
    "    W2 = pm.Normal(\"W2\", 0, 1, shape=(128, 10))\n",
    "    b2 = pm.Normal(\"b2\", 0, 1, shape=(10,))\n",
    "    \n",
    "    # Output layer (Softmax for classification)\n",
    "    logits = pm.math.dot(hidden, W2) + b2\n",
    "    y_obs = pm.Categorical(\"y_obs\", pm.math.softmax(logits), observed=y_train)\n",
    "    \n",
    "    # Variational Inference (ADVI)\n",
    "    approx = pm.fit(n=5000, method=\"advi\")  # Run ADVI optimization\n",
    "    trace = approx.sample(1000)  # Sample from the approximate posterior\n",
    "\n",
    "# Plot the ELBO loss curve\n",
    "plt.plot(approx.hist)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"ELBO Loss\")\n",
    "plt.title(\"Variational Inference Training\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319c1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
